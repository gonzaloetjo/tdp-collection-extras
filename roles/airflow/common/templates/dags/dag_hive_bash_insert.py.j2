from airflow import DAG
from airflow.operators.bash_operator import BashOperator
from airflow.utils.dates import days_ago
from datetime import datetime, timedelta

default_args = {
    'owner': 'tdp_user',
    'depends_on_past': False,
    'start_date': days_ago(1),
    'retry_delay': timedelta(minutes=5),
    'run_as_user': 'tdp_user',
    'queue':'edge',

}

dag = DAG(
    'hive_bash_insert_dag',
    default_args=default_args,
    description='A test Hive DAG to insert data into a table',
    schedule_interval=None,
)

create_database_hql = '''DROP DATABASE IF EXISTS tdp_user_dag_insert_test CASCADE;
                         CREATE DATABASE IF NOT EXISTS tdp_user_dag_insert_test 
                         LOCATION 'warehouse/tdp_user_dag_insert_test';
                    '''


create_database = BashOperator(
    task_id='create_database',
    bash_command=f'beeline -e "{create_database_hql}"',
    env={"KRB5CCNAME": "/tmp/krb5cc_1101"},    
    dag=dag,
)

create_table_hql = '''
                CREATE TABLE IF NOT EXISTS tdp_user_dag_insert_test.table_insert_test (
                    id INT,
                    name STRING
                )
                STORED AS PARQUET
                '''
create_table = BashOperator(
    task_id='create_table',
    bash_command=f'beeline -e "{create_table_hql}"',
    env={"KRB5CCNAME": "/tmp/krb5cc_1101"},
    dag=dag,
)

insert_data_hql = "INSERT INTO tdp_user_dag_insert_test.table_insert_test (id, name) VALUES (1, 'John Doe')"

insert_data = BashOperator(
    task_id='insert_data',
    bash_command=f'beeline -e "{insert_data_hql}"',
    env={"KRB5CCNAME": "/tmp/krb5cc_1101"},
    dag=dag,
)

select_table_hql = "SELECT * FROM tdp_user_dag_insert_test.table_insert_test LIMIT 3"

select_table = BashOperator(
    task_id='select_table',
    bash_command=f'beeline -e "{select_table_hql}"',
    env={"KRB5CCNAME": "/tmp/krb5cc_1101"},
    dag=dag,
)


create_database >> create_table >> insert_data >> select_table
