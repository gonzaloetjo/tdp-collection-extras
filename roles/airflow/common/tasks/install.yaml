---
- name: Ensure hadoop group exists
  include_role:
    name: tosit.tdp.utils.group
  vars:
    group: "{{ hadoop_group }}"

- name: Ensure airflow user exists
  include_role:
    name: tosit.tdp.utils.user
  vars:
    user: "{{ airflow_user }}"
    group: "{{ hadoop_group }}"

- name: Ensures {{ airflow_root_dir }} exists
  file:
    path: "{{ airflow_root_dir }}"
    state: directory
    owner: root
    group: root
    mode: "755"

- name: Ensures {{ airflow_root_dir }}/airflow-{{ airflow_version }} exists
  file:
    path: "{{ airflow_root_dir }}/airflow-{{ airflow_version }}"
    state: directory
    owner: root
    group: root
    mode: "755"

- name: Ensures log directory
  file:
    path: '{{ airflow_log_dir }}'
    state: directory
    owner: '{{ airflow_user }}'
    group: '{{ hadoop_group }}'

- name: Create directory for pid
  file:
    path: '{{ airflow_pid_dir }}'
    state: directory
    group: root
    owner: root
    mode: "755"

- name: Template airflow tmpfiles.d
  template:
    src: tmpfiles-airflow-webserver.conf.j2
    dest: /etc/tmpfiles.d/airflow-webserver.conf

- name: Create symbolic link to Airflow installation
  file:
    src: "{{ airflow_root_dir }}/airflow-{{ airflow_version }}"
    dest: "{{ airflow_install_dir }}"
    state: link

- name: Template Constraint file
  template:
    src: constraints-3.6.txt.j2
    dest: /tmp/constraints-3.6.txt

- name: Create configuration directory
  file:
    path: "{{ airflow_root_conf_dir }}"
    state: directory
    owner: root
    group: root
    mode: "755"

- name: Create certificates directory
  file:
    path: "{{ airflow_certs_folder }}"
    state: directory
    group: root
    owner: root
    mode: "0755"

- name: Install packages
  yum:
    name:
      - gcc-c++
      - python-devel.x86_64
      - python3-devel.x86_64
      - cyrus-sasl-devel.x86_64
      - krb5-devel
    state: present

- name: Pip install psycopg and redis #apparently needed not sure why
  pip:
    name: 
      - psycopg2
      - redis
    executable: pip3
    extra_args: --constraint /tmp/constraints-3.6.txt

- name: Pip install airflow[celery, kerberos]
  pip:
    name: "apache-airflow[celery, kerberos]=={{ airflow_version }}"
    executable: pip3
    extra_args: --constraint /tmp/constraints-3.6.txt
    state: present

- name: Pip install apache-hive provider
  pip:
    name: "apache-airflow-providers-apache-hive"
    executable: pip3
    extra_args: --constraint /tmp/constraints-3.6.txt

- name: Pip install apache-spark provider
  pip:
    name: "apache-airflow-providers-apache-spark"
    executable: pip3
    extra_args: --constraint /tmp/constraints-3.6.txt

- name: Template airflow.cfg file
  template:
    src: airflow.cfg.j2
    dest: "{{ airflow_install_dir }}/airflow.cfg"

# Dags: move this to a separate task later
- name: Create dag directory in /opt/tdp/airflow/airflow
  ansible.builtin.file:
    path: "{{ airflow_install_dir }}/dags"
    state: directory
    mode: '0755'
    owner: airflow
    group: hadoop

- name: Upload airflow dags templates
  template:
    src: "{{ item }}"
    dest: "{{ airflow_install_dir }}/dags/{{ item | basename | regex_replace('\\.j2$', '') }}"
    owner: "{{ airflow_user }}"
    group: "{{ hadoop_group }}"
  with_items:
    - dag_hive.py.j2
    - dag_hive_insert.py.j2
    - dag_hive_nyc.py.j2
    - dag_spark.py.j2
    - dag_spark_new.py.j2
    - dag_hive_new.py.j2

- name: Update .bashrc for Airflow user
  become: yes
  become_user: "{{ airflow_user }}"
  blockinfile:
    path: "~/.bashrc"
    block: |
      # Set environment variables for Airflow
      export AIRFLOW_HOME={{ airflow_install_dir }}
      export PATH=$PATH:/usr/local/bin
